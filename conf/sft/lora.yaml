accelerator:
  gradient_accumulation_steps: 4
informer:
  mode: pretrain
  data:
    wudao: 
      pattern: "/data/wudao/part-*.jsonl.zst"
      ratio: 0.1
    pubmedc: 
      pattern: "/data/pubmedc/part-*.jsonl.zst"
      ratio: 0.2
    wiki: 
      pattern: "/data/wiki/part-*.jsonl.zst"
      ratio: 0.1
  tokenzier:
    path: "/root/autodl-tmp/project/LLMs/Bloom-560m"
    special_tokens:
      pad_token: "<|extra_0|>"
      eos_token: "<|endoftext|>"
  max_seq_length: 2000
  batch_size: 8
  num_workers: 8
  prefetch_factor: 20
modeller:
  mode: loraeparameter
  args:
    ckpt: ""
    conf: ""
    target_modules: ["q_proj", "v_proj"]
    inference_mode: False
    rank: 1
    alpha: 32
    dropout: 0.1
  gradient_checkpointing_enable: True
  weight_decay: 1.0e-1
trainer:
  mode: sft
  epochs: 2
  lr: 2.0e-4
  log_steps: 100
  eval_steps: 10000
  save_steps: 10000
  warmup_steps: 1000
  experiments:
    exp_dir: "../experiments"
    name: "exp_20231201"
    weights: "weights"
    conf: "conf"
    load_model: False
    load_step: 1000
